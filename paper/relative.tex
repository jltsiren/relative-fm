\documentclass{llncs}
\usepackage{amsmath}


\newcommand{\BWT}
  {\ensuremath{\mathsf{BWT}}}
\newcommand{\rank}
  {\ensuremath{\mathsf{rank}}}
\newcommand{\select}
  {\ensuremath{\mathsf{select}}}


\begin{document}

\title{Relative FM-indexes}
\maketitle

\begin{abstract}
Intuitively, if two strings $S_1$ and $S_2$ are sufficiently similar and we already have an FM-index for $S_1$ then, by storing a little extra information, we should be able to reuse parts of that index in an FM-index for $S_2$.  We formalize this intuition and show that it can lead to significant space savings in practice, as well as to some interesting theoretical problems.
\end{abstract}


\section{Introduction}
\label{sec:introduction}

FM-indexes~\cite{FM05} are core components in most modern DNA aligners (e.g.,~\cite{LTPS09,LD09,LYLLYKW09}) and have thus played an important role in the genomics revolution.  Medical researchers are now producing databases of hundreds or even thousands of human genomes, so bioinformatics researchers are working to improve FM-indexes' compression of sets of nearly duplicate strings.  As far as we know, however, the solutions proposed so far (e.g.,~\cite{FGHP??,MNSV10}) index the concatenation of the genomes, so we can search the whole database easily but searching only in one specified genome is more difficult.  In this paper we consider how to index each of the genomes individually while still using reasonable space and query time.

Our intuition is that if two strings $S_1$ and $S_2$ are sufficiently similar and we already have an FM-index for $S_1$ then, by storing a little extra information, we should be able to reuse parts of that index in an FM-index for $S_2$.  More specifically, it seems $S_1$'s and $S_2$'s Burrows-Wheelers Transforms~\cite{BW94} (BWTs) should also be fairly similar.  The BWT sorts the characters of a string into the lexicographic order of the suffixes following those characters.  For example, if
\begin{align*}
S_1 & = \mathsf{GCACTTAGAGGTCAGT}\\
S_2 & = \mathsf{GCACTAGACGTCAGT}
\end{align*}
then
\begin{align*}
\BWT (S_1) & = \mathsf{TCTGCGTAAAAGGTGC}\\
\BWT (S_2) & = \mathsf{TGCTCGTAAAACGCG}
\end{align*}
whose longest common subsequence (LCS) {\sf TCTCGTAAAAGG} is nearly as long as either BWT.  We leave as an open problem proving bounds on the edit distance between \(\BWT (S_1)\) and \(\BWT (S_2)\) in terms of the edit distance between $S_1$ and $S_2$.
% FIXME The standard analysis on the effect of edit operations on the BWT of a repetitive collection
% applies here too. See Section 2.

The most important component of an FM-index for a string is a data structure supporting fast \rank\ queries on the string's BWT.  Suppose we store bitvectors $B_1$ and $B_2$ in which the 1s mark subsequences $D_1$ and $D_2$ complementary to an LCS $C$ in \(\BWT (S_1)\) and \(\BWT (S_2)\), respectively.  We claim that if we can support fast \rank\ queries on \(\BWT (S_1)\), $B_1$, $B_2$, $D_1$ and $D_2$ and fast $\select_0$ queries on $B_1$, then we can support fast \rank\ queries on \(\BWT (S_2)\).

To see why, notice that
\begin{align*}
\BWT (S_2).\rank_X (i)
& = C.\rank_X (B_2.\rank_0 (i))\\
& \quad + D_2.\rank_X (B_2.\rank_1 (i))
\end{align*}
and, by the same reasoning, 
\begin{align*}
C.\rank_X (j)
& = \BWT (S_1).\rank_X (B_1.\select_0 (j))\\
& \quad - D_1.\rank_X (B_1.\rank_1 (B_1.\select_0 (j)))\,.
\end{align*}
Therefore,
\begin{align*}
\BWT (S_2).\rank_X (i)
& = \BWT (S_1).\rank_X (k)\\
& \quad - D_1.\rank_X (B_1.\rank_1 (k))\\
& \quad + D_2.\rank_X (B_2.\rank_1 (i))
\end{align*}
where \(k = B_1.\select_0 (B_2.\rank_0 (i))\).

In our example, with
\begin{align*}
B_1 & = 0001000000000111\\
B_2 & = 010000000001010\\
D_1 & = \mathsf{GTGC}\\
D_2 & = \mathsf{GCC}
\end{align*}
we have \(B_1.\select_0 (B_2.\rank_0 (13)) = 12\) so
\begin{align*}
\BWT (S_2).\rank_\mathsf{C} (13)
& = \BWT (S_1).\rank_\mathsf{C} (12)\\
& \quad - D_1.\rank_\mathsf{C} (B_1.\rank_1 (12))\\
& \quad + D_2.\rank_\mathsf{C} (B_2.\rank_1 (13))\\
& = 3\,.
\end{align*}
We invite the reader to try other \rank\ queries and other examples.

In this paper we are mainly concerned with the practical applications of our observation.  In Section~\ref{sec:simon&jouni} we describe how we built an FM-index for the genome of a Han Chinese individual, reusing a \rank\ data structure over the BWT of the human reference genome. The Han genome is about 3.0 billion base pairs, the reference is about 3.1 billion base pairs and we found a common subsequence of about 2.9 billion base pairs. A standard implementation of a stand-alone FM-index for the Han genome takes 628~MB or 1090~MB, depending on encoding, while our index uses only 256~MB or 288~MB on top of the index for the reference.  On the other hand, queries to our index take about 9.5 or 4.5 times longer.  Since our index is compressed relative to the underlying index for the reference, we call it a relative FM-index.

The \rank\ data structure over the BWT normally dominates the space usage of an FM-index because the other components all have sublinear size.  If we store enough relative FM-indexes and our space savings for the \rank\ data structures are good enough, however, those other components could become a concern.  In particular, the suffix array (SA) sample used for locating and extracting usually takes an only slightly sublinear number of bits, unless those operations are slow.  (Bitvectors with constant-time queries can also take quite a lot of space, but that can be reduced by increasing the query times.)  We have developed (but not yet tested) a way to reuse an SA sample --- see the appendix for an overview --- but it involves finding a long common subsequence in \(\BWT (S_1)\) and \(\BWT (S_2)\) such that the relative order of its characters in $S_1$ and $S_2$ is the same.  We show in Section~\ref{sec:djamal&giovanni} that finding the longest such subsequence is NP-hard, but we also give heuristics that seem 
to work well in practice. 


\section{Simon \& Jouni}
\label{sec:simon&jouni}

Finding the longest common subsequence of two long strings is expensive. To make the construction of a relative FM-index practical, we approximate the LCS of the two Burrows-Wheeler transforms, using the combinatorial properties of the BWT to align the sequences.

Let $S_{1}$ be a random string of length $n$ over alphabet $\Sigma$ of size $\sigma$, and let string $S_{2}$ differ from it by $s$ insertions, deletions, and substitutions. In the expected case, the edit operations move $O(s \log_{\sigma} n)$ suffixes in lexicographic order, and change the preceding characters for $O(s)$ suffixes~\cite{MNSV10}. If we remove the characters corresponding to those suffixes from $\BWT(S_{1})$ and $\BWT(S_{2})$, we have a common subsequence of length $n - O(s \log_{\sigma} n)$ in the expected case.

Assume that we have partitioned the BWTs according to the first $k$ characters of the suffixes, for $k \ge 0$. For all $x \in \Sigma^{k}$, let $\BWT_{x}(S_{1})$ and $\BWT_{x}(S_{2})$ be the substrings of the BWTs corresponding to the suffixes starting with $x$. If we remove the suffixes affected by the edit operations, as well as the suffixes where string $x$ covers an edit, we have a common subsequence $\BWT_{x}'$ of $\BWT_{x}(S_{1})$ and $\BWT_{x}(S_{2})$. If we concatenate the sequences $\BWT_{x}'$ for all $x$, we get a common subsequence of $\BWT(S_{1})$ and $\BWT(S_{2})$ of length $n - O(s (k + \log_{\sigma} n))$ in the expected case. This suggests that we can find a long common subsequence of $\BWT(S_{1})$ and $\BWT(S_{2})$ by partitioning the BWTs, finding LCS for each partition, and concatenating the results.

In practice, we partition the BWTs by variable-length strings. We use backward searching on the BWTs to traverse the suffix trees of $S_{1}$ and $S_{2}$, selecting a partition when either the length of $\BWT_{x}(S_{1})$ or $\BWT_{x}(S_{2})$ is at most $1024$, or the length of the pattern $x$ reaches $32$. For each partition, we use the greedy LCS algorithm~\cite{Myers86} to find the longest common subsequence of that partition. To avoid hard cases, we stop the greedy algorithm if it would need diagonals beyond $\pm 50000$, and match only the most common characters for that partition. We also predict in advance the common cases where this happens (the difference of the lengths of $\BWT_{x}(S_{1})$ and $\BWT_{x}(S_{2})$ is over $50000$, or $x = N^{32}$ for DNA sequences), and match the most common characters in that partition directly.

We implemented the counting structure of the relative FM-index using the SDSL library~\cite{Gog2014b}, and compared its performance to a regular FM-index. To encode the BWTs and sequences $D_{1}$ and $D_{2}$, we used Huffman-shaped wavelet trees with either plain or entropy-compressed (RRR)~\cite{Raman2007} bitvectors. We chose entropy-compressed bitvectors for marking the positions of the LCS in $\BWT(S_{1})$ and $\BWT(S_{2})$.

The implementation was written in C++ and compiled on g++ version 4.7.3. We used a system with 32 gigabyes of memory and two quad-core 2.53 GHz Intel Xeon E5540 processors, running Ubuntu 12.04 with Linux kernel 3.2.0. Only one CPU core was used in the experiments.

For our experiments, we used the 1000 Genomes Project assembly of the human reference genome as the reference sequence $S_{1}$.\footnote{GRCh37, \url{ftp://ftp-trace.ncbi.nih.gov/1000genomes/ftp/technical/reference/}} As sequence $S_{2}$, we used the genome of a Han Chinese individual from the YanHuang project.\footnote{\url{ftp://public.genomics.org.cn/BGI/yanhuang/fa/}} The lengths of the sequences were 3.10 billion bases and 3.00 billion bases, respectively, and our algorithm found a common subsequence of 2.93 billion bases. As our pattern set, we used 10 million reads of length 56. Almost 4.20 million reads had exact matches in sequence $S_{2}$, with a total of 99.7 million occurrences. The results of the experiments can be seen in Table~\ref{table:experiments}.

\begin{table}
\centering
\caption{Experiments with human genomes. Bitvector used in the wavelet tree; time and space requirements for building the relative FM-index; time required for counting queries and index size for a regular and a relative FM-index; the performance of the relative FM-index compared to the regular index. The query times are averages over five runs.}\label{table:experiments}
\begin{tabular}{ccccccccccccc}
\hline
\noalign{\smallskip}
 & & \multicolumn{2}{c}{Construction} & & \multicolumn{2}{c}{Regular} & & \multicolumn{2}{c}{Relative} & & \multicolumn{2}{c}{Rel vs.~Reg} \\
Bitvector & & Time & Space & & Time & Size & & Time & Size & & Time & Size \\
\noalign{\smallskip}
\hline
\noalign{\smallskip}
Plain & &  762 s & 9124 MB & & 146 s & 1090 MB & & 1392 s & 288 MB & & 954 \% & 26 \% \\
\noalign{\smallskip}
RRR   & & 6022 s & 7823 MB & & 667 s &  628 MB & & 3022 s & 256 MB & & 453 \% & 41 \% \\
\noalign{\smallskip}
\hline
\end{tabular}
\end{table}

% FIXME Should we mention that it is easy to extract the regular index from the relative index?
With plain bitvectors in the wavelet tree, the relative FM-index was 9.5 times slower than a regular FM-index, while requiring a quarter of the space. With entropy-compressed bitvectors, the relative index was 4.5 times slower and required 41~\% of the space. Comparing the relative FM-index using plain bitvectors to the regular index using entropy-compressed bitvectors, we see that the relative index is 2.1 times slower, while taking 46~\% of the space.

% FIXME We have numbers. Do we want to show them?
Bitvectors $B_{1}$ and $B_{2}$ took 70~\% to 80~\% of the total size of the relative index. We tried to encode them as sparse bitvectors~\cite{Okanohara2007}, but the result was slightly larger and clearly slower than with entropy-compressed bitvectors. By our estimates, run-length encoded bitvectors would have taken slightly more space than sparse vectors. Hybrid bitvectors using different encodings for different parts of the bitvector~\cite{Kaerkkaeinen2014} could improve compression, but the existing implementation does not work with vectors longer than $2^{31}$ bits.


\section{Djamal \& Giovanni}
\label{sec:djamal&giovanni}

As mentioned in Section~\ref{sec:introduction}, we have developed a way to reuse an SA sample, but it involves finding a long common subsequence in \(\BWT (S_1)\) and \(\BWT (S_2)\) such that the relative order of its characters in $S_1$ and $S_2$ is the same.  This is, of course, equivalent to finding a long common subsequence in $S_1$ and $S_2$ such that the relative order of its characters is \(\BWT (S_1)\) and \(\BWT (S_2)\) is the same.  Unfortunately, as we now show, it is NP-complete to determine even whether there is an LCS of $S_1$ and $S_2$ with this property, even when they are over a ternary alphabet.

Clearly we can check in polynomial time whether a given subsequence of $S_1$ and $S_2$ has this property, so the problem is in NP.  To show that it is NP-hard, we reduce from the NP-complete problem of permutation pattern matching~\cite{BBL98}, for which we are given two permutations $\pi_1$ and $\pi_2$ over $n$ and \(m \leq n\) elements, respectively, and asked to determine whether there is a subsequence of $\pi_1$ of length $m$ such that the relative order of the elements in that subsequence is the same as the relative order of the elements in $\pi_2$.  For example, if \(\pi_1 = 6, 3, 2, 1, 4, 5\) and \(\pi_2 = 4, 2, 1, 3\), then \(6, 2, 1, 4\) is such a subsequence.  

Specifically, we set
\begin{align*}
S_1 & = \mathsf{A B^{\pi_1 [1]} A B^{\pi_1 [2]} \cdots A B^{\pi_1 [n]}}\\
S_2 & = \mathsf{A C^{\pi_2 [1]} A C^{\pi_2 [2]} \cdots A C^{\pi_2 [m]}}\,,
\end{align*}
so the unique LCS of $S_1$ and $S_2$ is $\mathsf{A}^m$.  For our example,
\begin{align*}
S_1 & = \mathsf{A B^6 A B^3 A B^2 A B A B^5} = \mathsf{A B B B B B B A B B B A B B A B A B B B B B}\\
S_2 & = \mathsf{A C^4 A C^2 A C A C^3} = \mathsf{A C C C C A C C A C A C C C}\,.
\end{align*}
The BWT sorts the $m$ copies of {\sf A} in $S_2$ according to $\pi_2$ and sorts any subsequence of $m$ copies of {\sf A} in $S_1$ according to the corresponding subsequence of $\pi_1$.  Therefore, there is an LCS of $S_1$ and $S_2$ such that the relative order of its characters is \(\BWT (S_1)\) and \(\BWT (S_2)\) is the same, if and only if there is a subsequence of $\pi_1$ of length $m$ such that the relative order of the elements in that subsequence is the same as the relative order of the elements in $\pi_2$.

\begin{center}
\noindent{\Huge Heuristics?}
\end{center}

\bibliographystyle{plain}
\bibliography{relative}


\appendix


\section{Reusing an SA Sample}
\label{sec:sample}

As mentioned in Section~\ref{sec:introduction}, an FM-index for $S_1$ usually has an SA sample that takes an only slightly sublinear number of bits.  This sample has two parts: the first consists of a bitvector $R$ with 1s marking the positions in \(\BWT (S_1)\) of every $r$th character in $S_1$, and an array $A$ storing a mapping from the ranks of those characters' positions in \(\BWT (S_1)\) to their positions in $S_1$; the second is an array storing a mapping from the ranks of those characters' positions in $S$ to their positions in \(\BWT (S_1)\).  With these, given the position of a sampled character in \(\BWT (S_1)\), we can find its position in $S_1$, and vice versa.

These parts are used for locating and extracting queries, respectively, and the worst-case query times are proportional to $r$.  On the other hand, the size of the sample in words is proportional to the length of $S$ divided by $r$.  For details on how the sample works, we direct the reader to the full description of FM-indexes~\cite{FM05}.  We note only that if we sample irregularly, then the worst-case query times for locating and extracting are proportional to the maximum distance in $S$ between two consecutive sampled characters.  We leave consideration of extracting for the full version of the paper --- it is nearly symmetric to locating --- so we do not discuss the second part of the sample here.

In our example from Section~\ref{sec:introduction}, the characters of \(\BWT (S_1) [1..16]\) and \(\BWT (S_2) [1..15]\) are mapped to their positions by the BWT from
\begin{align*}
& S_1 [16, 2, 6, 8, 13, 1, 12, 3, 7, 9, 14, 10, 15, 5, 11, 4]\\
& S_2 [15, 7, 2, 5, 12, 1, 11, 8, 3, 6, 13, 9, 14, 4, 10]
\end{align*}
respectively.  (Notice the lists of indices are just the SAs of $S_1\$$ and $S_2\$$ with each value decremented.)  Therefore, if \(r = 3\) then
\begin{align*}
R & = 1000110010010001\\
A [1..6] & = [16, 13, 1, 7, 10, 4]\,.
\end{align*}
Comparing $R$ and
\[B_1 = 0001000000000111\]
we see that the sampled characters \(\BWT (S_1) [1, 5, 6, 9, 12]\) that are in $C$, are $C$'s 1st, 4th, 5th, 8th and 11th characters.  From
\[B_2 = 010000000001010\]
we see that the 1st, 4th, 5th, 8th and 11th characters in $C$ in \(\BWT (S_2)\) are \(\BWT (S_2) [1, 5, 6, 9, 13]\), which are mapped to their positions by the BWT from \(S_2 [15, 12, 1, 3, 14]\).

The relative order \(5, 3, 1, 2, 4\) of the positions \(15, 12, 1, 3, 14\) in $S_2$ of these characters, is {\em almost} the same as the relative order \(5, 4, 1, 2. 3\) of the positions \(16, 13, 1, 7, 10\) in $S_1$ of the sampled characters in \(\BWT (S_1)\) that are in $C$, which seems promising.  What if we choose $C$ and its occurrences in \(\BWT (S_1)\) and \(\BWT (S_2)\) such that the relative order in $S_1$ of all \(\BWT (S_1)\)'s characters that are in $C$, is the same as the relative order in $S_2$ of all \(\BWT (S_2)\)'s characters that are in $C$?

For example, we can choose instead
\begin{align*}
C' & = \mathsf{TCTCGTAAAGG}\\
B_1' & = 0001000001010101\\
B_2' & = 010000010001010\\
D_1' & = \mathsf{GAGTC}\\
D_2' & = \mathsf{GACC}
\end{align*}
even though $C'$ is not then an LCS of \(\BWT (S_1)\) and \(\BWT (S_2)\) and, thus, our data structures for supporting \rank\ in \(\BWT (S_2)\) are slightly larger.  With these choices, the characters in \(\BWT (S_1)\) and \(\BWT (S_2)\) that are in $C'$, are mapped to their positions by the BWT from
\begin{align*}
& S_1 [16, 2, 6, 13, 1, 12, 3, 7, 14, 15, 11]\\
& S_2 [15, 2, 5, 12, 1, 11, 3, 6, 13, 14, 10]
\end{align*}
and the relative order \(11, 2, 4, 8, 1, 7, 3, 5, 9, 10, 6\) of the indices in those two lists is the same, as desired.

Suppose we store yet another pair of bitvectors
\begin{align*}
M_1 & = 0001100111000000\\
M_2 & = 000100111000000
\end{align*}
with 1s marking the positions in $S_1$ and $S_2$ of characters that are not mapped into $C'$ in \(\BWT (S_1)\) and \(\BWT (S_2)\).  We claim that if we can support fast \rank\ queries on $B_2'$, $R$ and $M_1$, fast access to $A$ and fast $\select_0$ queries on $B_1'$ and $M_2$, then we can support fast access to a (possibly irregular) sample SA sample for $S_2$ with as many sampled characters as there are in $C'$ in \(\BWT (S_1)\).  More specifically, if \(\BWT (S_2) [i]\) is in $C'$ and
\[R [B_1'.\select_0 (B_2'.\rank_0 (i))] = 1\]
--- meaning the corresponding character in $C'$ in \(\BWT (S_1)\) is sampled --- then \(\BWT (S_2) [i]\) is mapped to its position by the BWT from
\[S_2 \left[ \rule{0ex}{6ex}
    M_2.\select_0 \left( \rule{0ex}{4.5ex}
      M_1.\rank_0 \left( \rule{0ex}{4ex}
        A \left[ \rule{0ex}{3ex}
          R.\rank_1 \left( \rule{0ex}{2.5ex}
            B_1'.\select_0 \left( \rule{0ex}{2ex}
              B_2'.\rank_0 (i)
            \right)
          \right)
        \right]
      \right)
    \right)
  \right]\,.\]
We leave a detailed explanation to the full version of this paper.  We note, however, that this approach works for any sample rate $r$, and even if the SA sample for $S_1$ is irregular itself.

In our example, since \(\BWT (S_2) [10]\) is in $C'$, \(B_1'.\select_0 (B_2'.\rank_0 (10)) = 9\) and \(R [9] = 1\), we know \(\BWT (S_2) [10]\) is mapped to its position by the BWT from position
\[M_2.\select_0 \left(
    M_1.\rank_0 \left( \rule{0ex}{2ex}
      A [R.\rank_1 (9)]
    \right)
  \right)
= 6\]
in \(S_2\).


\end{document}