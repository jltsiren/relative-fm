
\section{Introduction}

The \emph{suffix tree} \cite{Weiner1973} is one of the most powerful bioinformatic tools to
answer complex queries on DNA and protein sequences \cite{Gus97,Ohl13,MBCT15}.
A serious problem that hampers its wider use on large genome sequences is its
size, which may be 10--20 bytes per character. In addition, the non-local
access patterns required by most interesting problems solved with suffix trees
complicate secondary-memory deployments. This problem has led to numerous
efforts to reduce the suffix tree size by representing it as a set of
\emph{compact data structures} \cite{Sadakane2007,Fischer2009a,Ohlebusch2009,Ohlebusch2010,Russo2011,Gog2011a,Abeliuk2013,Navarro2014a}, leading to
\emph{compressed suffix trees} (\CST). Currently, the smallest
\CST{} is the so-called \FCST{} \cite{Russo2011,Navarro2014a}, which uses about
5 \emph{bits} per character (bpc) but takes milliseconds to simulate suffix
tree navigation operations. In the other extreme, Sadakane's \CST{}
\cite{Sadakane2007} takes about 12~bpc and operates in microseconds, and even
nanoseconds for the simplest operations.

A space usage of 12~bpc may seem reasonable to handle, for example, one human
genome, which has about 3.1 billion bases: it can be operated within a
RAM of 4.5~GB (the representation contains the sequence as well). However, the
times where a single genome was analyzed are quickly giving way to databases
of genomes. Sequencing is becoming a routine activity where big companies
produce thousands of genomes per day, and initiatives like the \emph{1000 Genomes
Project} \cite{1000GP} for challenging the current techniques with
multigenome-scale data are flourishing.

Just storing 1000 human genomes using a 12~bpc \CST{} requires almost 4.5~TB, which
is not a RAM size found in a commodity server. Even considering powerful
commodity servers of 256~GB of RAM, we would need a cluster of 17 servers to
handle such a collection of \CST{}s (compared to over 100 with classical suffix
tree implementations!). With the much smaller (and much slower) \FCST, this would
drop to almost 7 servers. It is clear that further space reductions in the
representation of compressed suffix trees would lead to reductions in hardware, communication,
and energy costs when implementing complex searches over large genomic
databases.

An important characteristic of those large genome databases is that they
usually consist of the genomes of individuals of the same or closely related
species. This implies that the collections are highly \emph{repetitive}, that
is, each genome can be obtained by concatenating a few pieces of other genomes
and adding a few new characters. When repetitiveness is considered, much higher
compression rates can be obtained in compressed suffix trees. For example, it is possible to reduce
the space to 1--2~bpc (albeit with operation times in the milliseconds)
\cite{Abeliuk2013}, or to 2--3~bpc with operation times in the microseconds
\cite{Navarro2014}. For example, using 2~bpc, our 1000 genomes could be handled
with just 3 servers of 256~GB of RAM. We note, however, that these \CST{}s index
the whole collection and not individual sequences, which makes a difference on
the types of queries that can be answered, and also make a distributed
implementation less obviously scalable.

Compression algorithms best capture repetitiveness by using \emph{grammar}
compression or \emph{Lempel-Ziv} compression.\footnote{We refer to ``long-range''
repetitiveness, where similar texts may be found far away in the text
collection.} In the first case \cite{KY00,CLLPPSS05} one finds a context-free
grammar that generates (only) the text collection, and the grammar is smaller
as the collection is more repetitive. Rather than directly applying it on the
text, the current \CST{}s for repetitive collections \cite{Abeliuk2013,Navarro2014}
apply grammar compression on the data structures that simulate the suffix tree.
Grammar compression yields relatively easy direct access to the compressed
sequence \cite{BLRSRW15}. This makes it attractive compared to Lempel-Ziv
compression \cite{ZL77}, despite the latter generally achieving less space.

Lempel-Ziv compression cuts the collection into \emph{phrases}, each of which
has already appeared before. To extract the content of a phrase, one may have
to recursively extract the content at that earlier position, in a possibly long
chain of indirections.
Indeed, the only indexes built on Lempel-Ziv compression \cite{KN13} or on
combinations of Lempel-Ziv and grammar compression \cite{GGKNP12,GGKNP14,GP15}
support only pattern matching, which is just one of the wide range of
functionalities offered by suffix trees. The inability to access the data
at random positions lies at the heart of the research on indexes built on
Lempel-Ziv compression.

A simple way out of this limitation is the so-called \emph{relative Lempel-Ziv}
(\RLZ) compression \cite{Kuruppu2010}, where one of the sequences is represented
in plain form and all the others can take phrases only from that reference
sequence. This enables immediate access for the symbols inside any copied
phrase (as no transitive referencing exists) and, at least in cases where a
good reference sequence is found, offers competitive compression compared with
classical Lempel-Ziv. In our case, taking any random genome per species as the
references is good enough; more sophisticated techniques have been studied
\cite{KPZ11}. Structures for direct access \cite{DG11,Ferrada2014}
and even for pattern matching \cite{DJSS14,Belazzougui2014} have been developed
on top of \RLZ.

In this paper we show that \RLZ{} is sufficiently friendly as a compression format
to build a \CST{} on it, which uses less space as repetitiveness increases.
On a collection of human genomes, we obtain as little as 3~bpc and operate
within microseconds. This performance is comparable to that of a previous \CST{}
for this scenario \cite{Navarro2014}, but our \CST{}s have a different
functionality: we have a separate \CST{} for each sequence, instead of a single
\CST{} for their concatenation. Depending on the application, one kind of \CST{} or
the other is necessary.

Our compressed suffix tree, called \RCST, follows a trend of \CST{}s
\cite{Fischer2009a,Ohlebusch2009,Ohlebusch2010,Gog2011a,Abeliuk2013} that use only a pattern-matching index
(called \emph{suffix array}) and an array with the length of the longest common prefix
between each suffix and the previous one in lexicographic order (called $\mLCP$).
We use a suffix array based on relative compression \cite{Belazzougui2014}, and
compress $\mLCP$ using \RLZ. On top of the $\mLCP$ phrases we build a tree
of range minima that enables fast queries for range minimum queries, next and
previous smaller values, on $\mLCP$ \cite{Abeliuk2013}. All the \CST{} functionality
is built on those queries \cite{Fischer2009a}. Our main algorithmic contribution
is this \RLZ\nobreakdash-based representation of the $\mLCP$ array with the required extra
functionality.

\iffalse
The main topic of the paper will be the relative CST, but we also have the RLZ
bitvector and the relative FM-index for read collections. The RLZ bitvector is
a nice idea that works well in practice, if we can just find applications for
it. Sorting suffixes in lexicographic order amplifies the differences between
the sequences, so it's probably going to be something unrelated to suffix
trees and suffix arrays.

Relative data compression is a well-established topic. Version control systems
store
revisions of files as insertions and deletions to earlier revisions. In
bioinformatics, individual
genomes are often represented by listing their differences to the reference
genome
of the same species. More generally, we can use relative Lempel-Ziv (RLZ)
parsing [10]
to represent a text as a concatenation of substrings of a related text.

Compressed data structures for repetitive data. Given similar datasets
S1,...,Sr, the
data structure D(S1,...,Sr) is often repetitive. If we compress these
repetitions, we can
represent and use the data structure in much smaller space. Compressed data
structures
achieve better compression than relative data structures, because they can
take advantage
of the redundancy between all datasets, instead of just between the current
dataset and
the reference dataset. The price is less flexibility, as the encoding of each
dataset may
depend on all the other datasets. While the construction of compressed data
structures for
multiple datasets requires dedicated algorithms and often also significant
computational
resources, we can easily distribute the construction of relative data
structures to multiple
systems, as well as add and remove datasets.

Persistent data structures preserve the state of the data structure before
each operation.
Relative data structures can be seen as a special case of persistent data
structures
that preserves only the initial and the final state, with more emphasis on
space-e"ciency.
Also, while research on persistent data structures concentrates on structures
that can be
dynamically updated, my emphasis is on static data structures that are smaller
and faster
to use.
\fi

