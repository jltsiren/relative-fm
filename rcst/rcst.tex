\documentclass[a4paper,11pt]{llncs}
\usepackage{fullpage}
%\documentclass[10pt]{llncs}

\usepackage{graphicx}
\usepackage{url}

\usepackage[utf8]{inputenc}

\usepackage{amsmath}
\usepackage{amssymb}


% Mathematics
\newcommand{\set}[1]{\ensuremath{\{ #1 \}}}
\newcommand{\abs}[1]{\ensuremath{\lvert #1 \rvert}}
\renewcommand{\complement}[1]{\ensuremath{\overline{ #1 }}}

% Suffix trees
\newcommand{\ST}{\textsf{ST}}
\newcommand{\CST}{\textsf{CST}}
\newcommand{\CSTsada}{\textsf{CST\nobreakdash-Sada}}
\newcommand{\FCST}{\textsf{FCST}}
\newcommand{\CSTnpr}{\textsf{CST\nobreakdash-NPR}}
\newcommand{\RCST}{\textsf{RCST}}

% Suffix arrays etc.
\newcommand{\SA}{\textsf{SA}}
\newcommand{\ISA}{\textsf{ISA}}
\newcommand{\BWT}{\textsf{BWT}}
\newcommand{\CSA}{\textsf{CSA}}
\newcommand{\FMI}{\textsf{FMI}}
\newcommand{\RFM}{\textsf{RFM}}
\newcommand{\mSA}{\ensuremath{\mathsf{SA}}}
\newcommand{\mISA}{\ensuremath{\mathsf{ISA}}}
\newcommand{\mBWT}{\ensuremath{\mathsf{BWT}}}
\newcommand{\mF}{\ensuremath{\mathsf{F}}}
\newcommand{\mCSA}{\ensuremath{\mathsf{CSA}}}
\newcommand{\mRFM}{\ensuremath{\mathsf{RFM}}}

% LCP arrays
\newcommand{\LCP}{\textsf{LCP}}
\newcommand{\DLCP}{\textsf{DLCP}}
\newcommand{\PLCP}{\textsf{PLCP}}
\newcommand{\RLCP}{\textsf{RLCP}}
\newcommand{\LCPbyte}{\textsf{LCP\nobreakdash-byte}}
\newcommand{\LCPdac}{\textsf{LCP\nobreakdash-dac}}
\newcommand{\mLCP}{\ensuremath{\mathsf{LCP}}}
\newcommand{\mDLCP}{\ensuremath{\mathsf{DLCP}}}
\newcommand{\mPLCP}{\ensuremath{\mathsf{PLCP}}}
\newcommand{\mRLCP}{\ensuremath{\mathsf{RLCP}}}

% Other structures
\newcommand{\WT}{\textsf{WT}}
\newcommand{\mWT}{\ensuremath{\mathsf{WT}}}
\newcommand{\C}{\textsf{C}}
\newcommand{\mC}{\ensuremath{\mathsf{C}}}
\newcommand{\RLZ}{\textsf{RLZ}}
\newcommand{\mRLZ}{\ensuremath{\mathsf{RLZ}}}
\newcommand{\sdarray}{\textsf{sdarray}}
\newcommand{\LCS}{\textsf{LCS}}
\newcommand{\mLCS}{\ensuremath{\mathsf{LCS}}}
\newcommand{\mCS}{\ensuremath{\complement{\mathsf{LCS}}}}
\newcommand{\mleft}{\ensuremath{\mathsf{left}}}
\newcommand{\mright}{\ensuremath{\mathsf{right}}}

% Queries
\newcommand{\LF}{\textsf{LF}}
\newcommand{\find}{\textsf{find}}
\newcommand{\locate}{\textsf{locate}}
\newcommand{\extract}{\textsf{extract}}
\newcommand{\rank}{\textsf{rank}}
\newcommand{\select}{\textsf{select}}
\newcommand{\nsv}{\textsf{nsv}}
\newcommand{\nsev}{\textsf{nsev}}
\newcommand{\psv}{\textsf{psv}}
\newcommand{\psev}{\textsf{psev}}
\newcommand{\rmq}{\textsf{rmq}}

% Operators
\newcommand{\mLF}{\ensuremath{\mathsf{LF}}}
\newcommand{\mPsi}{\ensuremath{\mathsf{\Psi}}}
\newcommand{\mfind}{\ensuremath{\mathsf{find}}}
\newcommand{\mlocate}{\ensuremath{\mathsf{locate}}}
\newcommand{\mextract}{\ensuremath{\mathsf{extract}}}
\newcommand{\mrank}{\ensuremath{\mathsf{rank}}}
\newcommand{\mselect}{\ensuremath{\mathsf{select}}}
\newcommand{\mlcp}{\ensuremath{\mathsf{lcp}}}
\newcommand{\mpsv}{\ensuremath{\mathsf{psv}}}
\newcommand{\mpsev}{\ensuremath{\mathsf{psev}}}
\newcommand{\mnsv}{\ensuremath{\mathsf{nsv}}}
\newcommand{\mnsev}{\ensuremath{\mathsf{nsev}}}
\newcommand{\mrmq}{\ensuremath{\mathsf{rmq}}}
\newcommand{\Oh}{\ensuremath{\mathsf{O}}}
\newcommand{\oh}{\ensuremath{\mathsf{o}}}
\newcommand{\Th}{\ensuremath{\mathsf{\Theta}}}

% CST operations
\newcommand{\mRoot}{\ensuremath{\mathsf{Root}}}
\newcommand{\mLeaf}{\ensuremath{\mathsf{Leaf}}}
\newcommand{\mAncestor}{\ensuremath{\mathsf{Ancestor}}}
\newcommand{\mCount}{\ensuremath{\mathsf{Count}}}
\newcommand{\mLocate}{\ensuremath{\mathsf{Locate}}}
\newcommand{\mParent}{\ensuremath{\mathsf{Parent}}}
\newcommand{\mFChild}{\ensuremath{\mathsf{FChild}}}
\newcommand{\mNSibling}{\ensuremath{\mathsf{NSibling}}}
\newcommand{\mLCA}{\ensuremath{\mathsf{LCA}}}
\newcommand{\mSDepth}{\ensuremath{\mathsf{SDepth}}}
\newcommand{\mTDepth}{\ensuremath{\mathsf{TDepth}}}
\newcommand{\mLAQ}{\ensuremath{\mathsf{LAQ}}}
\newcommand{\mSLink}{\ensuremath{\mathsf{SLink}}}
\newcommand{\mChild}{\ensuremath{\mathsf{Child}}}
\newcommand{\mLetter}{\ensuremath{\mathsf{Letter}}}

% Bits
\newcommand{\onebit}{$1$\nobreakdash-bit}
\newcommand{\zerobit}{$0$\nobreakdash-bit}


\title{Relative Compressed Suffix Trees\thanks{This work is funded in part by:
by Fondecyt Project 1-140796; Basal Funds FB0001, Conicyt, Chile;
by Academy of Finland grants 258308 and 250345 (CoECGR); and by the Wellcome Trust grant [098051].}}

\author{
Travis Gagie\inst{1}
\and
Gonzalo Navarro\inst{2} %\fnmsep
%\thanks{Funded in part by Fondecyt Project 1-140796, Chile, and Basal Funds FB0001, Conicyt, Chile.}
\and
Simon J. Puglisi\inst{1}
\and
Jouni Sir\'en\inst{3} %\fnmsep
%\thanks{Funded by the Jenny and Antti Wihuri Foundation, Finland, and Basal Funds FB0001, Conicyt, Chile.}
}

\institute{
    Department of Computer Science,
    University of Helsinki, Finland\\
    \email{\{gagie,puglisi\}@cs.helsinki.fi}\\[1ex]
\and
    Center for Biotechnology and Bioengineering, Department of Computer Science,
    University of Chile, Chile\\
    \email{gnavarro@dcc.uchile.cl}\\[1ex]
\and
    Wellcome Trust Sanger Institute, United Kingdom\\
    \email{jouni.siren@sanger.ac.uk}\\[1ex]
}

\date{}


\pagestyle{plain}

\begin{document}

\maketitle

\begin{abstract}
\iffalse
This work investigates the use of mutual information between data structures for similar
datasets to represent the structures in less space. If two data structures are similar to each
other, one of them can probably be represented by its differences to the other, while still
supporting efficient queries. Such relative data structures may find use in bioinformatics,
where the genomes of individuals of the same species are very similar to each other.
More formally, assume that we have similar datasets R and S. If we build data
structure D for the datasets, we will likely see that D(R) and D(S) have low relative
entropy. Given D(R), we can probably represent D(S | R) (denoting D(S) relative to
dataset R) in small space, while still supporting the functionality of D efficiently. Then,
given D(R) and D(S | R), we can either simulate D(S) directly, or decompress it for
faster queries. A similar approach may also allow the construction of D(S) and D(S | R)
efficiently, given D(R), R, and the differences between datasets S and R.
Our work clearly has links to persistent data structures and can be thought of as a
special case where only the initial state and the final state are preserved, the final
state being the net result of potentially many individual modifications, all of which would
be represented by a persistent data structure.
\fi
\end{abstract}

\section{Introduction}

\iffalse
The main topic of the paper will be the relative CST, but we also have the RLZ bitvector and the relative FM-index for read collections. The RLZ bitvector is a nice idea that works well in practice, if we can just find applications for it. Sorting suffixes in lexicographic order amplifies the differences between the sequences, so it's probably going to be something unrelated to suffix trees and suffix arrays.

Relative data compression is a well-established topic. Version control systems store
revisions of files as insertions and deletions to earlier revisions. In bioinformatics, individual
genomes are often represented by listing their differences to the reference genome
of the same species. More generally, we can use relative Lempel-Ziv (RLZ) parsing [10]
to represent a text as a concatenation of substrings of a related text.

Compressed data structures for repetitive data. Given similar datasets S1,...,Sr, the
data structure D(S1,...,Sr) is often repetitive. If we compress these repetitions, we can
represent and use the data structure in much smaller space. Compressed data structures
achieve better compression than relative data structures, because they can take advantage
of the redundancy between all datasets, instead of just between the current dataset and
the reference dataset. The price is less flexibility, as the encoding of each dataset may
depend on all the other datasets. While the construction of compressed data structures for
multiple datasets requires dedicated algorithms and often also significant computational
resources, we can easily distribute the construction of relative data structures to multiple
systems, as well as add and remove datasets.

Persistent data structures preserve the state of the data structure before each operation.
Relative data structures can be seen as a special case of persistent data structures
that preserves only the initial and the final state, with more emphasis on space-e"ciency.
Also, while research on persistent data structures concentrates on structures that can be
dynamically updated, my emphasis is on static data structures that are smaller and faster
to use.
\fi

\begin{itemize}
\item motivation for indexing, suffix trees
\item CSAs, CSTs
\item repetitive data
\item relative data structures
\item this paper: relative CST
\end{itemize}


\section{Background}

A \emph{string} $S[1,n] = s_{1} \dotso s_{n}$ is a sequence of \emph{characters} over an \emph{alphabet} $\Sigma = \set{1, \dotsc, \sigma}$. For indexing purposes, we often consider \emph{text} strings $T[1,n]$ that are terminated by an \emph{endmarker} $T[n] = \$ = 0$ not occurring elsewhere in the text. \emph{Binary} sequences are sequences over alphabet $\set{0,1}$. If $B[1,n]$ is a binary sequence, its \emph{complement} is binary sequence $\complement{B}[1,n]$, with $\complement{B}[i] = 1 - B[i]$.

For any binary sequence $B[1,n]$, we define the \emph{subsequence} $S[B]$ of string $S[1,n]$ as the concatenation of characters $s_{i}$ with $B[i] = 1$. The complement of subsequence of $S[B]$ is the subsequence $S[\complement{B}]$. Mirroring the set notation, we will often write $S[\alpha \mid \beta]$ to denote the subsequence of characters at positions determined by expression $\alpha$ under condition $\beta$. Contiguous subsequences $S[i,j]$ are called \emph{substrings}. Substrings of type $S[1,j]$ and $S[i,n]$ are called \emph{prefixes} and \emph{suffixes}, respectively. We define the \emph{lexicographic order} among strings in the usual way.

\subsection{Full-text indexes}

The \emph{suffix tree} (\ST) \cite{Weiner1973} of text $T$ is a trie containing the suffixes of $T$, with unary paths compacted into single edges. As there are no unary internal nodes in the suffix tree, there can be at most $2n-1$ nodes, and the suffix tree can be stored in $\Oh(n \log n)$ bits. In practice, this is at least $10n$ bytes for small texts \cite{Kurtz1999}, and more for large texts as the pointers grow larger. If $v$ is a node of a suffix tree, we write $\pi(v)$ to denote the label of the path from the root to node $v$.

\emph{Suffix arrays} (\SA) \cite{Manber1993} were introduced as a space-efficient alternative to suffix trees. The suffix array $\mSA[1,n]$ of text $T$ is an array of pointers to the suffixes of the text in lexicographic order. In its basic form, the suffix array requires $n \log n$ bits in addition to the text, but its functionalities are more limited than those of the suffix tree. In addition to the suffix array, many algorithms also use the \emph{inverse suffix array} $\mISA[1,n]$, with $\mSA[\mISA[i]] = i$ for all $i$.

Let $\mlcp(S_{1}, S_{2})$ be the length of the longest common prefix of strings $S_{1}$ and $S_{2}$. The \emph{longest-common-prefix} (\LCP) \emph{array} \cite{Manber1993} $\mLCP[1,n]$ of text $T$ stores the lengths of lexicographically adjacent suffixes of $T$ as $\mLCP[i] = \mlcp(T[\mSA[i-1],n], T[\mSA[i],n])$. Let $v$ be an internal node of the suffix tree, $\ell = \abs{\pi(v)}$ the \emph{string depth} of node $v$, and $\mSA[sp,ep]$ the suffix array interval corresponding to node $v$. The following properties hold for the \LCP{} \emph{interval} $\mLCP[sp,ep]$: i) $\mLCP[sp] < \ell$; ii) $\mLCP[i] \ge \ell$ for all $sp < i \le ep$; iii) $\mLCP[i] = \ell$ for at least one $sp < i \le ep$; and iv) $\mLCP[ep+1] < \ell$ \cite{Abouelhoda2004}.

Abouelhoda et al.~\cite{Abouelhoda2004} simulated the suffix tree by using the suffix array, the \LCP{} array, and a representation of the suffix tree topology based on the \LCP{} intervals, paving way for more space-efficient suffix tree representations.

\subsection{Compressed text indexes}

Sequences supporting \rank{} and \select{} queries are the main building block of compressed text indexes. If $S$ is a sequence, we define $\mrank_{c}(S,i)$ to be the number of occurrences of character $c$ in the prefix $S[1,i]$, and $\mselect_{c}(S,j)$ is the position of the occurrence of rank $j$ in sequence $S$. A \emph{bitvector} is a representation of a binary sequence $B$ supporting fast \rank{} and \select{} queries. \emph{Wavelet trees} (\WT) \cite{Grossi2003} use bitvectors to support \rank{} and \select{} on strings.

The \emph{Burrows-Wheeler transform} (\BWT) \cite{Burrows1994} is a reversible permutation $\mBWT[1,n]$ of text $T$. It is defined as $\mBWT[i] = T[\mSA[i] - 1]$ (with $\mBWT[i] = T[n]$, if $\SA[i] = 1$). Originally intended for data compression, the Burrows-Wheeler transform has been widely used in space-efficient text indexes, because it shares the combinatorial structure of the suffix tree and the suffix array.

Let \LF{} be a function such that $\mSA[\mLF(i)] = \mSA[i] - 1$ (with $\mSA[\mLF(i)] = n$, if $\mSA[i] = 1$). We can compute $\mLF$ as $\mLF(i) = \mC[\mBWT[i]] + \mrank_{\mBWT[i]}(\mBWT, i)$, where $\mC[c]$ is the number of occurrences of characters with lexicographical values smaller than $c$ in \BWT. The inverse function of \LF{} is $\mPsi$, with $\mPsi(i) = \mselect_{c}(\mBWT, i - \mC[c])$, where $c$ is the largest character value with $\mC[c] < i$. With functions \LF{} and $\mPsi$, we can move forward and backward in the text, while maintaining the lexicographic rank of the current suffix. If sequence $S$ is not evident from the context, we write $\mLF_{S}$ and $\mPsi_{S}$.

\emph{Compressed suffix arrays} (\CSA) \cite{Ferragina2005a,Grossi2005} are text indexes supporting similar functionality as the suffix array. This includes the following queries: i) $\mfind(P) = [sp,ep]$ finds the lexicographic range of suffixes starting with \emph{pattern} $P[1,\ell]$; ii) $\mlocate(sp,ep) = \mSA[sp,ep]$ locates these suffixes in the text; and iii) $\mextract(i,j) = T[i,j]$ extracts substrings of the text. In practice, the \find{} performance of compressed suffix arrays can be competitive with suffix arrays, while \locate{} queries are orders of magnitude slower \cite{Ferragina2009a}. Typical index sizes are less than the size of the uncompressed text.

The \emph{FM-index} (\FMI) \cite{Ferragina2005a} is a common type of compressed suffix arrays. A typical implementation stores the \BWT{} in a wavelet tree. \find{} queries are supported by \emph{backward searching}. Let $[sp,ep]$ be the lexicographic range of suffixes starting with the suffix $P[i+1,\ell]$ of the pattern. We can find the range matching the suffix $P[i,\ell]$ with a generalization of function \LF{} as
$$
\mLF([sp,ep],P[i]) =
[\mC[P[i]] + \mrank_{P[i]}(\mBWT, sp-1) + 1,
\mC[P[i]] + \mrank_{P[i]}(\mBWT, ep)].
$$

We support \locate{} queries by \emph{sampling} some suffix array pointers. If we want to determine $\mSA[i]$ that has not been sampled, we can compute it as $\mSA[i] = \mSA[j]+k$, where $\mSA[j]$ is a sampled pointer found by iterating \LF{} $k$ times, starting from $i$. Given \emph{sample interval} $d$, the samples can be chosen in \emph{suffix order}, sampling $\mSA[i]$ at positions divisible by $d$, or in \emph{text order}, sampling $T[i]$ at positions divisible by $d$ and marking the sampled \SA{} positions in a bitvector. Suffix order sampling requires less space, often resulting in better time/space trade-offs, while text order sampling guarantees better worst-case performance. \extract{} queries are supported by sampling some \ISA{} pointers. To extract $T[i,j]$, we find the nearest sampled pointer after $\mISA[j]$, and traverse backwards to $T[i]$ with function \LF.

\begin{table}
\centering{}
\caption{Typical compressed suffix tree operations.}\label{table:cst operations}

\begin{tabular}{ll}
\hline
\noalign{\smallskip}
\textbf{Operation}  & \textbf{Description} \\
\noalign{\smallskip}
\hline
\noalign{\smallskip}
$\mRoot()$          & The root of the tree. \\
$\mLeaf(v)$         & Tells whether node $v$ a leaf. \\
$\mAncestor(v,w)$   & Tells whether node $v$ is an ancestor of node $w$. \\
\noalign{\smallskip}
$\mCount(v)$        & Number of leaves in the subtree with $v$ as the root. \\
$\mLocate(v)$       & Pointer to the suffix corresponding to leaf $v$. \\
\noalign{\smallskip}
$\mParent(v)$       & The parent of node $v$. \\
$\mFChild(v)$       & The first child of node $v$ in alphabetic order. \\
$\mNSibling(v)$     & The next sibling of node $v$ in alphabetic order. \\
$\mLCA(v,w)$        & The lowest common ancestor of nodes $v$ and $w$. \\
\noalign{\smallskip}
$\mSDepth(v)$       & String depth: Length $\ell = \abs{\pi(v)}$ of the label from the root to node $v$. \\
$\mTDepth(v)$       & Tree depth: The depth of node $v$ in the suffix tree. \\
$\mLAQ_{S}(v,d)$    & The highest ancestor of node $v$ with string depth at least $d$. \\
$\mLAQ_{T}(v,d)$    & The ancestor of node $v$ with tree depth $d$. \\
\noalign{\smallskip}
$\mSLink(v)$        & Suffix link: Node $w$ such that $\pi(v) = c \pi(w)$ for a character $c \in \Sigma$. \\
$\mSLink^{k}(v)$    & Suffix link iterated $k$ times. \\
\noalign{\smallskip}
$\mChild(v,c)$      & The child of node $v$ with edge label starting with character $c$. \\
$\mLetter(v,i)$     & The character $\pi(v)[i]$. \\
\noalign{\smallskip}
\hline
\end{tabular}
\end{table}

\emph{Compressed suffix trees} (\CST) \cite{Sadakane2007} are compressed text indexes supporting the full functionality of a suffix tree (see Table~\ref{table:cst operations}). They combine a compressed suffix array, a compressed representation of the \LCP{} array, and a compressed representation of suffix tree topology. For the \LCP{} array, there are several common representations:
\begin{itemize}
\item \LCPbyte{} \cite{Abouelhoda2004} stores the \LCP{} array as a byte array. If $\mLCP[i] < 255$, the \LCP{} value is stored in the byte array. Larger values are marked with a $255$ in the byte array and stored separately. Because most \LCP{} values are typically small, \LCPbyte{} usually requires $n$ to $1.5n$ bytes of space.
\item We can store the \LCP{} array by using variable-length codes. \LCPdac{} uses \emph{directly addressable codes} \cite{Brisaboa2009} for the purpose, resulting in a structure that is typically somewhat smaller and somewhat slower than \LCPbyte.
\item The \emph{permuted} \LCP{} (\PLCP) \emph{array} \cite{Sadakane2007} $\mPLCP[1,n]$ is the \LCP{} array stored in text order and used as $\mLCP[i] = \mPLCP[\mSA[i]]$. Because $\mPLCP[i+1] \ge \mPLCP[i]-1$, the array can be stored as a bitvector of length $2n$ in $2n+\oh(n)$ bits. If the text is repetitive, run-length encoding can be used to compress the bitvector to even less space \cite{Fischer2009a}. Because accessing \PLCP{} uses \locate, it is much slower than the other common encodings.
\end{itemize}

Tree topology representations are the main differences between the various \CST{} proposals. While various compressed suffix arrays and \LCP{} arrays are interchangeable, tree topology determines how various suffix tree operations are implemented. There are three main families of compressed suffix trees:
\begin{itemize}
\item \emph{Sadakane's compressed suffix tree} (\CSTsada) \cite{Sadakane2007} uses a \emph{balanced parentheses} representation for the tree. Each node is encoded as an opening parenthesis, followed by the encodings of its children and finally a closing parenthesis. This can be encoded as a bitvector of length $2n'$ for a tree with $n'$ nodes, requiring up to $4n+\oh(n)$ bits. \CSTsada{} tends to be larger and faster than the other compressed suffix trees \cite{Gog2011a,Abeliuk2013}.
\item The \emph{fully compressed suffix tree} (\FCST) of Russo et al.~\cite{Russo2011,Navarro2014a} aims to use as little space as possible. It does not require an \LCP{} array at all, and stores a balanced parentheses representation for Navarro2014a sampled subset of suffix tree nodes in $\oh(n)$ bits. Unsampled nodes are retrieved by following suffix links. \FCST{} is smaller and much slower than the other compressed suffix trees \cite{Russo2011,Abeliuk2013}.
\item Fischer et al.~\cite{Fischer2009a,Ohlebusch2010,Gog2011a,Abeliuk2013} proposed an intermediate representation, \CSTnpr, based on \LCP{} intervals. Tree navigation is handled by searching for the values defining the \LCP{} intervals. \emph{Range minimum queries} $\mrmq(sp,ep)$ find the leftmost minimal value in $\mLCP[sp,ep]$, while \emph{next/previous smaller value} queries $\mnsv(i)$ and $\mpsv(i)$ find the next/previous \LCP{} value smaller than $\mLCP[i]$.
\end{itemize}

For typical texts and component choices, the size of compressed suffix trees ranges from the $1.5n$ to $3n$ bytes of \CSTsada{} to the $0.5n$ to $n$ bytes of \FCST{} \cite{Gog2011a,Abeliuk2013}. There are also some \CST{} variants for repetitive texts, such as versioned document collections and collections of individual genomes. Abeliuk et al.~\cite{Abeliuk2013} developed a variant of \CSTnpr{} that can be smaller than $n$ bits, while achieving similar performance as the \FCST. Navarro and Ordóñez \cite{Navarro2014} used grammar-based compression for the tree representation of \CSTsada, resulting in a compressed suffix tree that requires slightly more space than the \CSTnpr{} of Abeliuk et al., while being closer to the non-repetitive \CSTsada{} and \CSTnpr{} in performance.

\subsection{Relative Lempel-Ziv}

\emph{Relative Lempel-Ziv} (\RLZ) parsing \cite{Kuruppu2010} compresses \emph{target} sequence $S$ relative to \emph{reference} sequence $R$. The target sequence is represented as a concatenation of $z$ \emph{phrases} $w_{i} = (p_{i}, \ell_{i}, c_{i})$, where $p_{i}$ is the starting position of the phrase in the reference, $\ell_{i}$ is the length of the copied substring, and $c_{i}$ is the \emph{mismatching} character. If phrase $w_{i}$ starts from position $p'$ in the target, then $S[p',p'+\ell_{i}-1] = R[p_{i},p_{i}+\ell_{i}-1]$ and $S[p'+\ell_{i}] = c_{i}$.

% FIXME citation?
The shortest \RLZ{} parsing of the target sequence can be found in linear time \cite{???}. The algorithm builds a \CSA{} for the reverse of the reference sequence, and then parses the target sequence greedily by using backward searching. If the edit distance between the reference and the target is $s$, we need at most $s$ phrases to represent the target sequence. On the other hand, because the relative order of the phrases can be different in sequences $R$ and $S$, the edit distance can be much higher than the number of phrases in the shortest \RLZ{} parsing.

In a straightforward implementation, the \emph{phrase pointers} $p_{i}$ and the mismatching characters $c_{i}$ can be stored in arrays $W_{p}$ and $W_{c}$. These arrays take $z \log \abs{S}$ bits and $z \log \sigma$ bits, respectively. To support random access in the target sequence, we can encode phrase lengths as bitvector $W_{\ell}$ of length $\abs{S}$ \cite{Kuruppu2010}. We set $W_{\ell}[j] = 1$, if $S[j]$ is the first character of a phrase. The bitvector requires $z \log \frac{n}{z} + \Oh(z)$ bits, if we use the \sdarray{} representation \cite{Okanohara2007}. To extract $S[j]$, we first determine the phrase $w_{i}$, with $i = \mrank_{1}(W_{\ell}, j)$. If $W_{\ell}[j+1] = 1$, we return the mismatching character $W_{c}[i]$. Otherwise we determine the phrase offset with a \select{} query, and return character $R[W_{p}[i] + j - \mselect_{1}(W_{\ell}, i)]$.

The \select{} query can be avoided by using \emph{relative pointers} instead of absolute pointers \cite{Ferrada2014}. By setting $W_{p}[i] = p_{i} - \mselect_{1}(W_{\ell}, i)$, the general case simplifies to $S[j] = R[W_{p}[i] + j]$. If most of the differences between the reference and the target sequence are single-character \emph{substitutions}, $p_{i+1}$ will often be $p_{i} + \ell_{i} + 1$. This corresponds to $A_{p}[i+1] = W_{p}[i]$ with relative pointers, making \emph{run-length encoding} the pointer array worthwhile.


\section{Relative FM-index}

The \emph{relative FM-index} (\RFM) \cite{Belazzougui2014} is compressed suffix array of a sequence relative to the \CSA{} of another sequence. We write $\mRFM(S \mid R)$ to denote the relative FM-index of target sequence $S$ relative to reference sequence $R$. The index is based on approximating the \emph{longest common subsequence} (\LCS) of $\mBWT(R)$ and $\mBWT(S)$, and storing several structures based on the common subsequence. Given a representation of $\mBWT(R)$ supporting \rank{} and \select{}, we can use the relative index $\mRFM(S \mid R)$ to simulate \rank{} and \select{} on $\mBWT(S)$.

\subsection{Basic index}

Assume that we have found a long common subsequence of sequences $X$ and $Y$. We call positions $X[i]$ and $Y[j]$ \emph{lcs-positions}, if they are in the common subsequence. If $B_{X}$ and $B_{Y}$ are the binary sequences marking the common subsequence ($X[B_{X}] = Y[B_{Y}]$), we can map between the corresponding lcs-positions in the two sequences with \rank{} and \select{} operations. If $X[i]$ is an lcs-position, the corresponding position in sequence $Y$ is $Y[\mselect_{1}(B_{Y}, \mrank_{1}(B_{X}, i))]$. We denote this pair of \emph{lcs-bitvectors} $\mLCS(X,Y)$.

In its most basic form, relative FM-index $\mRFM(S \mid R)$ only supports \find{} queries by simulating \rank{} queries on $\mBWT(S)$. It does this by storing $\mLCS(\BWT(R),\BWT(S))$ and the complements $\mCS(R)$ and $\mCS(S)$ of the common subsequence. The lcs-bitvectors are compressed using \emph{entropy-based compression} \cite{Raman2007}, while the complements are stored in similar structures as the reference $\mBWT(R)$.

To compute $\mrank_{c}(\mBWT(S), i)$, we first determine the number of lcs-positions in $\mBWT(S)$ up to position $S[i]$ as $k = \mrank_{1}(B_{\mBWT(S)}, i)$. Then we find the lcs-position $k$ in $\mBWT(R)$ as $j = \mselect_{1}(B_{\mBWT(R)}, k)$. With these positions, we can compute
$$
\mrank_{c}(\mBWT(S), i) = \mrank_{c}(\mBWT(R), j) - \mrank_{c}(\mCS(R), j-k) + \mrank_{c}(\mCS(S), i-k).
$$

\subsection{Relative select}

We can implement the entire functionality of a compressed suffix array with \rank{} queries on the \BWT. However, if we use the \CSA{} in a compressed suffix tree, we also need \select{} queries to support \emph{forward searching} with $\mathsf{\mPsi}$ and $\mChild$ queries. We can always implement \select{} queries by binary searching with \rank{} queries, but the result will be much slower than \rank{} queries.

The faster alternative to support \select{} queries in the relative FM-index is to build a \emph{relative select} structure \cite{Boucher2015}. Let $\mF = \mF(X)$ be a sequence consisting of the characters of sequence $X$ in sorted order. Alternatively, $\mF$ is a sequence such that $\mF[i] = \mBWT[\mPsi(i)]$. The relative select structure consists of bitvectors $\mLCS(\mF(R), \mF(S))$, where $B_{\mF(R)}[i] = B_{\mBWT(R)}[\mPsi(i)]$ and $B_{\mF(S)}[i] = B_{\mBWT(S)}[\mPsi(i)]$, as well as the \C{} array $\mC(\mLCS)$ for the common subsequence.

To compute $\mselect_{c}(\mBWT(S), i)$, we start by determining how many of the first $i$ occurrences of character $c$ are lcs-positions as $k = \mrank_{1}(B_{\mF(S)}, \mC(\mBWT(S))[c] + i) - \mC(\mLCS)[c]$. Then we check from bit $B_{\mF(S)}[\mC(\mBWT(S))[c] + i]$ whether the occurrence we are looking for is an lcs-position or not. If the occurrence is an lcs-position, we find it in $\mBWT(R)$ as $j = \mselect_{c}(\mBWT(R), \mselect_{1}(B_{\mF(R)}, \mC(\mLCS)[c] + k))$, and then map $j$ to $\mselect_{c}(\mBWT(S), i)$ by using $\mLCS(\mBWT(R), \mBWT(S))$. Otherwise we find the occurrence in $\mCS(S)$ as $j = \mselect_{c}(\mCS(S), i-k)$, and then return $\mselect_{c}(\mBWT(S), i) = \mselect_{0}(B_{\mBWT(S)}, j)$.

\subsection{Full functionality}

If we want the relative FM-index to support \locate{} and \extract{} queries, we cannot build it from any common subsequence of $\mBWT(R)$ and $\mBWT(S)$. We need a \emph{bwt-invariant subsequence} \cite{Belazzougui2014}, where the relative order of the characters in the same in both the original sequences and their Burrows-Wheeler transforms.

\begin{definition}\label{def:bwt-invariant}
Let $X$ be a common subsequence of $\mBWT(R)$ and $\mBWT(S)$, and let $\mBWT(R)[i_{R}]$ and $\mBWT(S)[i_{S}]$ be the lcs-positions corresponding to $X[i]$. Subsequence X is bwt-invariant, if
$$
\mSA(R)[i_{R}] < \mSA(R)[j_{R}] \iff \mSA(S)[i_{S}] < \mSA(S)[j_{S}]
$$
for all positions $i, j \in \set{1, \dotsc, \abs{X}}$.
\end{definition}

In addition to the structures already mentioned, the full relative FM-index has another set of lcs-bitvectors, $\mLCS(R,S)$, which marks the bwt-invariant subsequence in the original sequences. If $\mBWT(R)[i_{R}]$ and $\mBWT(S)[i_{S}]$ are lcs-positions, we set $B_{R}[\mSA(R)[i_{R}]-1] = 1$ and $B_{S}[\mSA(S)[i_{S}]-1] = 1$.\footnote{For simplicity, we assume that the endmarker is not a part of the bwt-invariant subsequence. Hence $\mSA[i] > 1$ for all lcs-positions $\mBWT[i]$.} If the target sequence contains long \emph{insertions} not present in the reference, we may also want to include some \SA{} and \ISA{} samples for querying those regions.

To compute the answer to a $\mlocate(i)$ query, we start by iterating $\mBWT(S)$ backwards with \LF{} queries, until we find an lcs-position $\mBWT(S)[i']$ after $k$ steps. Then we map position $i'$ into the corresponding position $j'$ in $\mBWT(R)$ by using $\mLCS(\mBWT(R),\mBWT(S))$. Finally we determine $\mSA(R)[j']$ with a \locate{} query in the reference index, and map the result to $\mSA(S)[i']$ by using $\mLCS(R,S)$.\footnote{If $\mBWT(S)[i']$ and $\mBWT(R)[j']$ are lcs-positions, the corresponding lcs-positions in the original sequences are $S[\mSA(S)[i']-1]$ and $R[\mSA(R)[j']-1]$.} The result of the $\mlocate(i)$ query is $\mSA(S)[i']+k$.

The $\mISA(S)[i]$ access required for \extract{} queries is supported in a similar way. We find the lcs-position $S[i+k]$ for the smallest $k \ge 0$, and map it to the corresponding position $R[j]$ by using $\mLCS(R,S)$. Then we determine $\mISA(R)[j+1]$ by using the reference index, and map it back to $\mISA(S)[i+k+1]$ with $\mLCS(\mBWT(R),\mBWT(S))$. Finally we iterate $\mBWT(S)$ backwards $k+1$ steps with \LF{} queries to find $\mISA(S)[i]$.

\subsection{Finding bwt-invariant subsequence}

With the basic relative FM-index, we approximate the longest common subsequence of $\mBWT(R)$ and $\mBWT(S)$ by partitioning the \BWT{}s according to lexicographic contexts, finding the longest common subsequence for each pair of substrings in the partitioning, and concatenating the results. The algorithm is fast and easy to parallelize, while not using too much memory. As such, \RFM{} construction is practical for very large \BWT{}s.

To find a bwt-invariant subsequence, we start by \emph{matching} each suffix of the reference sequence with the lexicographically nearest suffixes of the target sequence. Unlike in the original algorithm \cite{Belazzougui2014}, we only match suffixes that are lexicographically adjacent in the \emph{mutual suffix array} of the two sequences.

\begin{definition}
Let $R$ and $S$ be two sequences, and let $\mSA = \mSA(RS)$ and $\mISA = \mISA(RS)$. The \emph{left match} of suffix $R[i,\abs{R}]$ is suffix $S[\mSA[\mISA[i]-1] - \abs{R}, \abs{S}]$, if $\mISA[i] > 1$ and $\mSA[\mISA[i]-1]$ points to a suffix of $S$ ($\mSA[\mISA[i]-1] > \abs{R}$). The \emph{right match} of suffix $R[i,\abs{R}]$ is suffix $S[\mSA[\mISA[i]+1] - \abs{R}, \abs{S}]$, if $\mISA[i] < \abs{RS}$ and $\mSA[\mISA[i]+1]$ points to a suffix of $S$.
\end{definition}

Instead of using the mutual suffix array, we can use $\mCSA(R)$, $\mCSA(S)$, and the \emph{merging bitvector} $B_{R,S}$ of length $\abs{RS}$. We set $B_{R,S}[i] = 1$, if $\mSA(RS)[i]$ points to a suffix of $S$. We can build the merging bitvector in $\Oh(\abs{S} \cdot t_{\mLF})$ time, where $t_{\mLF}$ is the time required by an \LF{} query, by extracting $S$ from $\mCSA(S)$ and backward searching for it in $\mCSA(R)$ \cite{Siren2009}. Suffix $R[i,\abs{R}]$ has a left (right) match, if $B_{R,S}[\mselect_{0}(B_{R,S}, \mSA(R)[i])-1] = 1$ ($B_{R,S}[\mselect_{0}(B_{R,S}, \mSA(R)[i])+1] = 1)$).

Our next step is building the \emph{match arrays} $\mleft$ and $\mright$, which correspond to the arrays $A[\cdot][2]$ and $A[\cdot][1]$ in the original algorithm. This is done by traversing $\mCSA(R)$ backwards from $\mISA(R)[\abs{R}] = 1$ using \LF{} queries and following the left and the right matches of the current suffix. During the traversal, we maintain invariant $j = \mSA(R)[i]$ with $(i,j) \leftarrow (i-1, \mLF_{R}(j))$. If suffix $R[i,\abs{R}]$ has a left (right) match, we use shorthand $l(i) = \mrank_{1}(B_{R,S}, \mselect_{0}(B_{R,S}, j)-1)$ ($r(i) = \mrank_{1}(B_{R,S}, \mselect_{0}(B_{R,S}, j)+1)$) to refer to its position in $\mCSA(S)$.

We say that suffixes $R[i,\abs{R}]$ and $R[i+1,\abs{R}]$ have the same left match, if $l(i) = \mLF_{S}(l(i+1))$. Let $R[i,\abs{R}]$ to $R[i+\ell,\abs{R}]$ be a maximal run of suffixes having the same left match, with suffixes $R[i,\abs{R}]$ to $R[i+\ell-1,\abs{R}]$ starting with the same characters as their left matches.\footnote{The first character of a suffix can be determined by using the $\mC$ array.} We find the left match of suffix $R[i,\abs{R}]$ as $i' = \mSA(S)[l(i)]$ by using $\mCSA(S)$, and set $\mleft[i,i+\ell-1] = [i',i'+\ell-1]$. The right match array $\mright$ is built in a similar way.

The match arrays require $2\abs{R} \log \abs{S}$ bits of space. If sequences $R$ and $S$ are similar, the runs in the arrays tend to be long. Hence we can run-length encode the match arrays to save space. The traversal takes $\Oh(\abs{R} \cdot (t_{\mLF} + t_{\mrank} + t_{\mselect}) + rd \cdot t_{\mLF})$ time, where $t_{\mrank}$ and $t_{\mselect}$ denote the time required by \rank{} and \select{} operations, $r$ is the number of runs in the two arrays, and $d$ is the suffix array sample interval in $\mCSA(S)$.\footnote{The time bound assumes text order sampling.}

The final step is finding the longest increasing subsequence $X$ of arrays $\mleft$ and $\mright$, which corresponds to a common subsequence of $R$ and $S$. More precisely, we want to find a binary sequence $B_{R}[1,\abs{R}]$, which marks the common subsequence in $R$, and an integer sequence $X$, which contains the positions of the common subsequence in $S$. The goal is to make sequence $X$ strictly increasing and as long as possible, with $X[\mrank_{1}(B_{R}, i)]$ being either $\mleft[i]$ or $\mright[i]$. This can be done in $\Oh(\abs{R} \log \abs{R})$ time with $\Oh(\abs{R} \log \abs{R})$ bits of additional working space with a straightforward modification of the dynamic programming algorithm for finding the longest increasing subsequence. While the dynamic programming tables can be run-length encoded, the time and space savings are negligible in practice.

As sequence $X$ is strictly increasing, we can convert it into binary sequence $B_{S}[1,\abs{S}]$, marking the values in sequence $X$ with \onebit{}s. Afterwards, we can consider binary sequences $B_{R}$ and $B_{S}$ as the lcs-bitvectors $\mLCS(R,S)$. Because every suffix of $R$ starts with the same character as its matches stored in the $\mleft$ and $\mright$ arrays, subsequences $R[B_{R}]$ and $S[B_{S}]$ are identical. As each suffix $R[i,\abs{R}]$ with $B_{R}[i] = 1$ is paired with its left match or right match in sequence $S$, no other suffix of $R$ or $S$ is lexicographically between the two paired suffixes.

For any $i$, let $i_{R} = \mselect_{1}(B_{R}, i)$ and $i_{S} = \mselect_{1}(B_{S}, i)$ be the lcs-positions of rank $i$. Then,
$$
\mISA(R)[i_{R}] < \mISA(R)[j_{R}] \iff \mISA(S)[i_{S}] < \mISA(S)[j_{S}]
$$
for any $i,j \le \abs{X}$, which is equivalent to the condition in Definition~\ref{def:bwt-invariant}. We can convert $\mLCS(R,S)$ to $\mLCS(\mBWT(R),\mBWT(S))$ in $\Oh((\abs{R}+\abs{S}) \cdot t_{\mLF})$ time by traversing $\mCSA(R)$ and $\mCSA(S)$ backwards. The resulting subsequence of $\mBWT(R)$ and $\mBWT(S)$ is bwt-invariant.

Note that the full relative FM-index is more limited than the basic index, because it does not handle \emph{substring moves} very well. Let $R = xy$ and $S = yx$, for two random sequences $x$ and $y$ of length $n/2$ each. Because $\mBWT(R)$ and $\mBWT(S)$ are very similar, we can expect to find a common subsequence of length almost $n$. On the other hand, the length of the longest bwt-invariant subsequence is around $n/2$, because we can either pair the suffixes of $x$ or the suffixes of $y$ in $R$ and $S$, but not both.


\section{Relative compressed suffix tree}

The \emph{relative compressed suffix tree} (\RCST) is a \CSTnpr{} of the target sequence relative to a \CST{} of the reference sequence. It consists of two major components: the relative FM-index and the relative \LCP{} (\RLCP) array. Optional relative select structures can be generated or loaded from disk to speed up algorithms based on forward searching. The \RLCP{} array is based on the \RLZ{} parsing, while the support for \nsv/\psv/\rmq{} queries is based on a minimum tree over the phrases.

\subsection{Relative \LCP{} array}

Given \LCP{} array $\mLCP[1,n]$, we define the \emph{differential} \LCP{} \emph{array} $\mDLCP[1,n]$ as $\mDLCP[1] = \mLCP[1]$ and $\mDLCP[i] = \mLCP[i] - \mLCP[i-1]$ for $i > 1$. If $\mBWT[i,j] = c^{j+1-i}$ for some $c \in \Sigma$, then $\mLCP[\mLF(i)+1,\mLF(j)]$ is the same as $\mLCP[i+1,j]$, with each value incremented by $1$ \cite{Fischer2009a}. This means $\mDLCP[\mLF(i)+2,\mLF(j)] = \mDLCP[i+2,j]$ making the \DLCP{} array of a repetitive text compressible with grammar-based compression \cite{Abeliuk2013}.

We make a similar observation in the relative setting. If target sequence $S$ is similar to the reference sequence $T$, then their \LCP{} arrays should also be similar. If there are long identical ranges $\mLCP(R)[i,i+k] = \mLCP(S)[j,j+k]$, the corresponding \DLCP{} ranges $\mDLCP(R)[i+1,i+k]$ and $\mDLCP(S)[j+1,j+k]$ are also identical. Hence we can use \RLZ{} parsing to compress either the original \LCP{} array or the \DLCP{} array.

While the identical ranges are a bit longer in the \LCP{} array, we opt to compress the \DLCP{} array, because it behaves better when there are long repetitions in the sequences. In particular, assembled genomes often have long runs of character $N$, which correspond to regions of very large \LCP{} values. If the runs are longer in the target sequence than in the reference sequence, the \RLZ{} parsing of the \LCP{} array will have many phrases consisting of only the mismatch character. The corresponding ranges in the \DLCP{} array typically consist of values $\set{-1, 0, 1}$, making them much easier to compress.

\begin{itemize}
\item reference as slarray (need fast random and sequential access)
\item RLZ compression; no pointer compression
\item random access using mismatches as samples
\item decompressing entire phrases
\end{itemize}

\subsection{Supporting \nsv/\psv/\rmq{} queries}

\begin{itemize}
\item minimum tree
\item encoding the tree
\item supporting queries
\item Also \nsev/\psev{} \cite{Abeliuk2013}.
\end{itemize}


\section{Experiments}

\begin{itemize}
\item environment
\item datasets
\item construction time, space (inexact)
\item component sizes; comparison to the old RFM
\item basic queries: LF, Psi, RMQ/PSV/NSV
\item locate() performance; note on sample rates with suffix order sampling
\item RCST size vs. mutation rate
\item RCST vs. CST for repetitive collections
\item CST traversal
\item maximal matches
\end{itemize}


\section{Conclusions}\label{section:conclusions}

\begin{itemize}
\item conclusions
\item other ideas: RLZ bitvectors, RLZ pointer compression
\item analyze the similarity between LCP arrays based on edit distances
\end{itemize}


\bibliographystyle{plain}
\bibliography{rcst}


\end{document}
